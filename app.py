"""app.py
æœ€å°å¯è¿è¡Œçš„ Streamlit åº”ç”¨ï¼š
- é€‰æ‹©èµ·å§‹å›¾ç‰‡æˆ–è¾“å…¥ä¸»é¢˜
- ç”Ÿæˆå¼€ç¯‡ â†’ çº¿ç´¢æå– â†’ å‘é‡æ£€ç´¢ â†’ ç»­å†™ï¼Œç›´è‡³è¾¾åˆ°ç›®æ ‡æ®µæ•°
- æ— å‘é‡åº“æˆ–æ— åŒ¹é…æ—¶ï¼Œé‡‡ç”¨å›é€€ç­–ç•¥ç»§ç»­æ–‡æœ¬ç”Ÿæˆ
"""
from __future__ import annotations

import json
from typing import List, Optional, Set, Tuple

import streamlit as st
from PIL import Image

from prompts import (
    SYSTEM_CLUE_EXTRACTOR,
    SYSTEM_STORY_WRITER,
    USER_CLUE_TEMPLATE,
    USER_CONTINUE_TEMPLATE,
    USER_OPENING_TEMPLATE,
)
from utils import (
    EmbeddingService,
    LLMClient,
    get_or_create_collection,
    list_image_paths,
    load_config,
    query_similar_documents,
    safe_open_image,
)


st.set_page_config(page_title="AI æ•…äº‹ç¼–ç»‡è€…", page_icon="ğŸ“–", layout="wide")

cfg = load_config()
llm = LLMClient(cfg)
emb = EmbeddingService(cfg.embedding_model_name)
collection = get_or_create_collection(cfg.chroma_persist_dir, name="images")


def extract_clues(paragraph: str) -> List[str]:
    """ä»æ®µè½ç»“å°¾æå– 1-3 ä¸ªå…³é”®è¯ï¼Œç”¨äºæ£€ç´¢ä¸‹ä¸€å¼ å›¾ç‰‡ã€‚"""
    prompt = USER_CLUE_TEMPLATE.format(paragraph=paragraph)
    text = llm.chat(SYSTEM_CLUE_EXTRACTOR, prompt).strip()
    try:
        arr = json.loads(text)
        if isinstance(arr, list):
            return [str(x) for x in arr][:3]
    except Exception:
        pass
    # å›é€€ï¼šå–æœ€åä¸€å¥ä¸­çš„åè¯æ ·å¼è¯ï¼ˆç®€å•è¿‘ä¼¼ï¼‰
    last = paragraph.split("ã€‚")[-2:]  # å–æœ«å°¾å¥å­ç‰‡æ®µ
    fallback = "ï¼Œ".join(last).strip()
    return [fallback[:16]] if fallback else ["çº¿ç´¢"]


def gen_opening(input_hint: str) -> str:
    user = USER_OPENING_TEMPLATE.format(input_hint=input_hint)
    return llm.chat(SYSTEM_STORY_WRITER, user).strip()


def gen_continuation(prev_paragraph: str, image_description: str) -> str:
    user = USER_CONTINUE_TEMPLATE.format(
        prev_paragraph=prev_paragraph, image_description=image_description
    )
    return llm.chat(SYSTEM_STORY_WRITER, user).strip()


def retrieve_next_image(clues: List[str]) -> Optional[Tuple[str, dict]]:
    """åŸºäºçº¿ç´¢å‘é‡æ£€ç´¢å€™é€‰å›¾ç‰‡ï¼Œè¿”å› (document, metadata)ã€‚æ‰¾ä¸åˆ°åˆ™ Noneã€‚"""
    query = "; ".join(clues)
    vec = emb.encode([query])[0]
    docs, metas = query_similar_documents(collection, vec, n_results=5)
    if not docs:
        return None
    # ç®€å•è¿”å›ç¬¬ä¸€ä¸ªå€™é€‰
    return docs[0], metas[0]


# ----------------------------
# UI
# ----------------------------

st.title("ğŸ“– AI æ•…äº‹ç¼–ç»‡è€…")

with st.sidebar:
    st.header("è®¾ç½®")
    images = list_image_paths(cfg.image_dir)
    image_names = [p.name for p in images]
    start_mode = st.radio("é€‰æ‹©å¼€ç«¯æ–¹å¼", ["é€‰æ‹©èµ·å§‹å›¾ç‰‡", "è¾“å…¥ä¸»é¢˜"], index=0)

    start_image_name = None
    start_topic = None
    if start_mode == "é€‰æ‹©èµ·å§‹å›¾ç‰‡":
        start_image_name = st.selectbox("èµ·å§‹å›¾ç‰‡", options=["(ä¸é€‰)"] + image_names, index=0)
    else:
        start_topic = st.text_input("æ•…äº‹ä¸»é¢˜", placeholder="ä¾‹å¦‚ï¼šé›¨åçš„åŸå¸‚ã€é—å¤±çš„è½¦ç¥¨â€¦â€¦")

    target_len = st.number_input("ç›®æ ‡æ®µè½æ•°", min_value=3, max_value=12, value=cfg.story_target_length, step=1)
    run = st.button("å¼€å§‹ç”Ÿæˆ", type="primary")

col_left, col_right = st.columns([1, 1])

# å±•ç¤ºå›¾ç‰‡åº“é¢„è§ˆ
with col_left:
    st.subheader("å›¾ç‰‡åº“é¢„è§ˆ")
    if images:
        thumbs = images[:8]
        for p in thumbs:
            img = safe_open_image(p)
            if img is not None:
                st.image(img, caption=p.name, use_column_width=True)
    else:
        st.info("è¯·å°†å›¾ç‰‡æ”¾å…¥ç›®å½•ï¼š" + cfg.image_dir)

# æ•…äº‹ç”ŸæˆåŒºåŸŸ
with col_right:
    st.subheader("ç”Ÿæˆç»“æœ")
    if run:
        used_paths: Set[str] = set()
        story: List[Tuple[str, Optional[str]]] = []  # (paragraph, image_path)

        # å¼€ç¯‡
        if start_mode == "é€‰æ‹©èµ·å§‹å›¾ç‰‡" and start_image_name and start_image_name != "(ä¸é€‰)":
            p = next((x for x in images if x.name == start_image_name), None)
            if p is not None:
                opening_hint = f"èµ·å§‹å›¾ç‰‡ï¼š{p.name}"
                first_paragraph = gen_opening(opening_hint)
                story.append((first_paragraph, str(p)))
                used_paths.add(str(p))
            else:
                first_paragraph = gen_opening(start_image_name)
                story.append((first_paragraph, None))
        else:
            topic = start_topic or "ä¸€ä¸ªæ™®é€šä½†ä¸å¹³å‡¡çš„å¤œæ™š"
            first_paragraph = gen_opening(topic)
            story.append((first_paragraph, None))

        # å¾ªç¯ç»­å†™
        while len(story) < int(target_len):
            last_paragraph = story[-1][0]
            clues = extract_clues(last_paragraph)
            candidate = retrieve_next_image(clues)

            if candidate is None:
                # å›é€€ï¼šæ— å›¾ä¹Ÿç»­å†™
                next_para = gen_continuation(last_paragraph, "ï¼ˆæ— æ–°å›¾ç‰‡ï¼‰")
                story.append((next_para, None))
                continue

            doc, meta = candidate
            image_path = meta.get("path")
            if image_path in used_paths:
                # é¿å…é‡å¤ï¼Œå›é€€
                next_para = gen_continuation(last_paragraph, doc)
                story.append((next_para, None))
                continue

            next_para = gen_continuation(last_paragraph, doc)
            story.append((next_para, image_path))
            used_paths.add(image_path)

        # å±•ç¤ºç»“æœ
        for idx, (para, img_path) in enumerate(story, 1):
            st.markdown(f"**ç¬¬ {idx} æ®µ**")
            if img_path:
                try:
                    st.image(Image.open(img_path), use_column_width=True)
                except Exception:
                    st.caption(f"[å›¾ç‰‡æ— æ³•æ˜¾ç¤º] {img_path}")
            st.write(para)
            st.divider()

        # å¯¼å‡ºï¼ˆç®€å•ï¼‰
        if st.download_button(
            label="å¯¼å‡º Markdown",
            data="\n\n".join([p for p, _ in story]).encode("utf-8"),
            file_name="story.md",
            mime="text/markdown",
        ):
            st.toast("å·²å¯¼å‡º story.md") 